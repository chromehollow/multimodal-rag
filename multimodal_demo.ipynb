{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f28f83-fbaf-456f-b6c4-c05e5ec869d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82cce59d-6c49-4446-af57-dcfbe7f0359b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from: assets/sample.txt\n",
      "Text extracted: Andre Achtar-Zadeh is a software engineer at OpenAI. He works on natural language models and AI systems.\n",
      " ...\n",
      "Initializing Qdrant and inserting vector...\n",
      "Running entity and relationship extraction...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "LLM response:\n",
      "{\n",
      "  \"entities\": [\"Andre Achtar-Zadeh\", \"software engineer\", \"OpenAI\", \"natural language models\", \"AI systems\"],\n",
      "  \"relationships\": [\n",
      "    [\"Andre Achtar-Zadeh\", \"works at\", \"OpenAI\"],\n",
      "    [\"Andre Achtar-Zadeh\", \"works on\", \"natural language models\"],\n",
      "    [\"Andre Achtar-Zadeh\", \"works on\", \"AI systems\"]\n",
      "  ]\n",
      "}\n",
      "Parsing extracted JSON data...\n",
      "Inserting entities and relationships into Neo4j...\n",
      "Running similarity search on Qdrant...\n",
      "Top vector result: id='e6f1c2b6-562b-4fd0-af79-0c805d8a9c1f' version=16 score=0.99999994 payload={'text': 'Andre Achtar-Zadeh is a software engineer at OpenAI. He works on natural language models and AI systems.\\n'} vector=None shard_key=None order_value=None\n",
      "Executing hybrid retrieval for: text test\n",
      "Running vector search...\n",
      "Running graph search...\n",
      "Vector Results: [' \\n72-Hour Technical Challenge \\nMultimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \\n \\nObjective \\nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \\nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \\nsearchable knowledge graph, and enables hybrid search using, keyword and vector \\nretrieval (Graph RAG System). \\nYou are expected to begin with evals to define success criteria and to structure your \\narchitecture accordingly. \\n \\nEnterprise Approach Highlights \\nBefore building any ingestion or pipeline logic, define your evaluation framework: \\n• \\nWhat constitutes a \"correct\" response? \\n• \\nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \\n• \\nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \\n• \\nHow will you fail gracefully? \\nStructure your work around a modular and scalable pipeline, including: \\n• \\nInput validation \\n• \\nQuery triage and rewriting \\n• \\nAgent-based retrieval orchestration \\n• \\nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \\nVector Retrieval \\n• \\nAnswer generation and post-processing \\n \\nChallenge Scope \\nBuild a multimodal RAG assistant that: \\n• \\nIngests at least three of the following modalities: text, image, audio, video \\n• \\nExtracts entities and relationships \\n• \\nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \\n• \\nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \\nsophisticated ingestion pipeline \\n• \\nPowers a hybrid search pipeline for fast and reliable access to domain-specific \\nknowledge \\n \\nRequirements \\n1. Evaluation-First Pipeline Design \\n• \\nDefine a minimal test suite using DeepEval or similar \\n• \\nClearly document: \\no Query types (lookup, summarization, semantic linkages) \\no Evaluation goals: retrieval quality, hallucination control, latency \\n• \\nInclude functional unit tests for each module \\n2. Data Ingestion and Preprocessing \\n• \\nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \\n• \\nModal-specific logic: \\no OCR/captioning \\no Transcription \\no Frame extraction and tagging for video \\n• \\nEnrich all outputs with metadata and domain tags \\n3. Entity & Relationship Extraction \\n• \\nUse LLMs to extract structured information \\n• \\nCross-modal linking of the same entity (e.g., “John Smith” in PDF + transcript) \\n• \\nGenerate or infer schema for graph database \\n6. User Interface / Demo \\n• \\nUI or notebook should support: \\no Uploading new files \\no Typing natural language queries \\no Viewing answers with optional graph exploration \\n• \\nLog evaluation output for each query \\n \\nBonus Features \\n• \\nScene detection for video \\n• \\nSentiment detection from text/audio \\n• \\nTopic-based reranking of results \\n• \\nReal-time feedback for query improvement \\n• \\nSecurity-aware design (query restrictions, access control) \\n \\nEvaluation Criteria \\n• \\nEnterprise fit: eval-first mindset, modularity, and clear architecture \\n• \\nPrecision and relevance: does the system retrieve the right context across \\nmodalities? \\n• \\nLatency: fast, efficient retrieval and generation \\n• \\nReliability: graceful failure handling and consistent outputs \\n• \\nMaintainability: clear logic, good documentation, testing \\nResources & Tips \\nThere’s no strict requirement for the type of dataset or topic. Suggested tools and sample \\nsets: \\nFrameworks \\n• \\nAutoGen: https://github.com/microsoft/autogen \\n• \\nCrewAI: https://github.com/joaomdmoura/crewAI \\n• \\nDeepEval: https://github.com/confident-ai/deepeval \\n• \\nArize Phoenix: https://github.com/Arize-ai/phoenix \\n• \\nLangChain: https://www.langchain.com/ \\n• \\nLlamaIndex: https://www.llamaindex.ai/ \\n• \\nChonkie: https://github.com/trychonkie/chonkie \\nMultimodal Tools \\n• \\nLLaVA: https://llava-vl.github.io/ \\nVector Databases \\n• \\nQdrant, Milvus, Weaviate \\nSample Datasets \\n• \\nDocVQA: https://docvqa.github.io/ \\n• \\nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \\n• \\nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \\n \\nSubmission \\nWithin 72 hours, submit: \\n• \\nGitHub repository or zip file \\n• \\nShort demo video (3–10 minutes) \\n• \\nLocal setup instructions \\n• \\nOptional: your evaluation report and reflections on architecture decisions \\n', ' \\n72-Hour Technical Challenge \\nMultimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \\n \\nObjective \\nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \\nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \\nsearchable knowledge graph, and enables hybrid search using, keyword and vector \\nretrieval (Graph RAG System). \\nYou are expected to begin with evals to define success criteria and to structure your \\narchitecture accordingly. \\n \\nEnterprise Approach Highlights \\nBefore building any ingestion or pipeline logic, define your evaluation framework: \\n• \\nWhat constitutes a \"correct\" response? \\n• \\nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \\n• \\nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \\n• \\nHow will you fail gracefully? \\nStructure your work around a modular and scalable pipeline, including: \\n• \\nInput validation \\n• \\nQuery triage and rewriting \\n• \\nAgent-based retrieval orchestration \\n• \\nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \\nVector Retrieval \\n• \\nAnswer generation and post-processing \\n \\nChallenge Scope \\nBuild a multimodal RAG assistant that: \\n• \\nIngests at least three of the following modalities: text, image, audio, video \\n• \\nExtracts entities and relationships \\n• \\nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \\n• \\nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \\nsophisticated ingestion pipeline \\n• \\nPowers a hybrid search pipeline for fast and reliable access to domain-specific \\nknowledge \\n \\nRequirements \\n1. Evaluation-First Pipeline Design \\n• \\nDefine a minimal test suite using DeepEval or similar \\n• \\nClearly document: \\no Query types (lookup, summarization, semantic linkages) \\no Evaluation goals: retrieval quality, hallucination control, latency \\n• \\nInclude functional unit tests for each module \\n2. Data Ingestion and Preprocessing \\n• \\nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \\n• \\nModal-specific logic: \\no OCR/captioning \\no Transcription \\no Frame extraction and tagging for video \\n• \\nEnrich all outputs with metadata and domain tags \\n3. Entity & Relationship Extraction \\n• \\nUse LLMs to extract structured information \\n• \\nCross-modal linking of the same entity (e.g., “John Smith” in PDF + transcript) \\n• \\nGenerate or infer schema for graph database \\n6. User Interface / Demo \\n• \\nUI or notebook should support: \\no Uploading new files \\no Typing natural language queries \\no Viewing answers with optional graph exploration \\n• \\nLog evaluation output for each query \\n \\nBonus Features \\n• \\nScene detection for video \\n• \\nSentiment detection from text/audio \\n• \\nTopic-based reranking of results \\n• \\nReal-time feedback for query improvement \\n• \\nSecurity-aware design (query restrictions, access control) \\n \\nEvaluation Criteria \\n• \\nEnterprise fit: eval-first mindset, modularity, and clear architecture \\n• \\nPrecision and relevance: does the system retrieve the right context across \\nmodalities? \\n• \\nLatency: fast, efficient retrieval and generation \\n• \\nReliability: graceful failure handling and consistent outputs \\n• \\nMaintainability: clear logic, good documentation, testing \\nResources & Tips \\nThere’s no strict requirement for the type of dataset or topic. Suggested tools and sample \\nsets: \\nFrameworks \\n• \\nAutoGen: https://github.com/microsoft/autogen \\n• \\nCrewAI: https://github.com/joaomdmoura/crewAI \\n• \\nDeepEval: https://github.com/confident-ai/deepeval \\n• \\nArize Phoenix: https://github.com/Arize-ai/phoenix \\n• \\nLangChain: https://www.langchain.com/ \\n• \\nLlamaIndex: https://www.llamaindex.ai/ \\n• \\nChonkie: https://github.com/trychonkie/chonkie \\nMultimodal Tools \\n• \\nLLaVA: https://llava-vl.github.io/ \\nVector Databases \\n• \\nQdrant, Milvus, Weaviate \\nSample Datasets \\n• \\nDocVQA: https://docvqa.github.io/ \\n• \\nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \\n• \\nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \\n \\nSubmission \\nWithin 72 hours, submit: \\n• \\nGitHub repository or zip file \\n• \\nShort demo video (3–10 minutes) \\n• \\nLocal setup instructions \\n• \\nOptional: your evaluation report and reflections on architecture decisions \\n', ' \\n72-Hour Technical Challenge \\nMultimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \\n \\nObjective \\nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \\nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \\nsearchable knowledge graph, and enables hybrid search using, keyword and vector \\nretrieval (Graph RAG System). \\nYou are expected to begin with evals to define success criteria and to structure your \\narchitecture accordingly. \\n \\nEnterprise Approach Highlights \\nBefore building any ingestion or pipeline logic, define your evaluation framework: \\n• \\nWhat constitutes a \"correct\" response? \\n• \\nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \\n• \\nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \\n• \\nHow will you fail gracefully? \\nStructure your work around a modular and scalable pipeline, including: \\n• \\nInput validation \\n• \\nQuery triage and rewriting \\n• \\nAgent-based retrieval orchestration \\n• \\nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \\nVector Retrieval \\n• \\nAnswer generation and post-processing \\n \\nChallenge Scope \\nBuild a multimodal RAG assistant that: \\n• \\nIngests at least three of the following modalities: text, image, audio, video \\n• \\nExtracts entities and relationships \\n• \\nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \\n• \\nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \\nsophisticated ingestion pipeline \\n• \\nPowers a hybrid search pipeline for fast and reliable access to domain-specific \\nknowledge \\n \\nRequirements \\n1. Evaluation-First Pipeline Design \\n• \\nDefine a minimal test suite using DeepEval or similar \\n• \\nClearly document: \\no Query types (lookup, summarization, semantic linkages) \\no Evaluation goals: retrieval quality, hallucination control, latency \\n• \\nInclude functional unit tests for each module \\n2. Data Ingestion and Preprocessing \\n• \\nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \\n• \\nModal-specific logic: \\no OCR/captioning \\no Transcription \\no Frame extraction and tagging for video \\n• \\nEnrich all outputs with metadata and domain tags \\n3. Entity & Relationship Extraction \\n• \\nUse LLMs to extract structured information \\n• \\nCross-modal linking of the same entity (e.g., “John Smith” in PDF + transcript) \\n• \\nGenerate or infer schema for graph database \\n6. User Interface / Demo \\n• \\nUI or notebook should support: \\no Uploading new files \\no Typing natural language queries \\no Viewing answers with optional graph exploration \\n• \\nLog evaluation output for each query \\n \\nBonus Features \\n• \\nScene detection for video \\n• \\nSentiment detection from text/audio \\n• \\nTopic-based reranking of results \\n• \\nReal-time feedback for query improvement \\n• \\nSecurity-aware design (query restrictions, access control) \\n \\nEvaluation Criteria \\n• \\nEnterprise fit: eval-first mindset, modularity, and clear architecture \\n• \\nPrecision and relevance: does the system retrieve the right context across \\nmodalities? \\n• \\nLatency: fast, efficient retrieval and generation \\n• \\nReliability: graceful failure handling and consistent outputs \\n• \\nMaintainability: clear logic, good documentation, testing \\nResources & Tips \\nThere’s no strict requirement for the type of dataset or topic. Suggested tools and sample \\nsets: \\nFrameworks \\n• \\nAutoGen: https://github.com/microsoft/autogen \\n• \\nCrewAI: https://github.com/joaomdmoura/crewAI \\n• \\nDeepEval: https://github.com/confident-ai/deepeval \\n• \\nArize Phoenix: https://github.com/Arize-ai/phoenix \\n• \\nLangChain: https://www.langchain.com/ \\n• \\nLlamaIndex: https://www.llamaindex.ai/ \\n• \\nChonkie: https://github.com/trychonkie/chonkie \\nMultimodal Tools \\n• \\nLLaVA: https://llava-vl.github.io/ \\nVector Databases \\n• \\nQdrant, Milvus, Weaviate \\nSample Datasets \\n• \\nDocVQA: https://docvqa.github.io/ \\n• \\nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \\n• \\nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \\n \\nSubmission \\nWithin 72 hours, submit: \\n• \\nGitHub repository or zip file \\n• \\nShort demo video (3–10 minutes) \\n• \\nLocal setup instructions \\n• \\nOptional: your evaluation report and reflections on architecture decisions \\n']\n",
      "Graph Results: []\n",
      "Pipeline execution complete. Results saved to results/hybrid_results.json\n"
     ]
    }
   ],
   "source": [
    "!python main.py assets/sample.txt \"text test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f88288a-a3d8-4798-a5a7-d1d7eabce03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from: assets/sample_image.png\n",
      "Text extracted: chrom,\n",
      "hollow”\n",
      " ...\n",
      "Initializing Qdrant and inserting vector...\n",
      "Running entity and relationship extraction...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "LLM response:\n",
      "{\n",
      "  \"entities\": [\"chrom\", \"hollow\"],\n",
      "  \"relationships\": []\n",
      "}\n",
      "Parsing extracted JSON data...\n",
      "Inserting entities and relationships into Neo4j...\n",
      "Running similarity search on Qdrant...\n",
      "Top vector result: id='51c27023-b442-45c9-a1e5-9e5338f8b465' version=18 score=0.99999994 payload={'text': 'chrom,\\nhollow”\\n'} vector=None shard_key=None order_value=None\n",
      "Executing hybrid retrieval for: png test\n",
      "Running vector search...\n",
      "Running graph search...\n",
      "Vector Results: ['chrom,\\nhollow”\\n', 'chrom,\\nhollow”\\n', 'chrom,\\nhollow”\\n']\n",
      "Graph Results: []\n",
      "Pipeline execution complete. Results saved to results/hybrid_results.json\n"
     ]
    }
   ],
   "source": [
    "!python main.py assets/sample_image.png \"png test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c1ec84-e71d-4a7d-8693-8ea0d5e58ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from: assets/sample_pdf.pdf\n",
      "Text extracted:  \n",
      "72-Hour Technical Challenge \n",
      "Multimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \n",
      " \n",
      "Objective \n",
      "Design and implement a modular prototype of an Enterprise Retrieval-Augmented \n",
      "Generation (RAG) system that supports text, image, audio, and video ingestion, builds a \n",
      "searchable knowledge graph, and enables hybrid search using, keyword and vector \n",
      "retrieval (Graph RAG System). \n",
      "You are expected to begin with evals to define success criteria and to structure your \n",
      "architecture a ...\n",
      "Initializing Qdrant and inserting vector...\n",
      "Running entity and relationship extraction...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "LLM response:\n",
      "{\n",
      "  \"entities\": [\"Enterprise Retrieval-Augmented Generation (RAG) system\", \"knowledge graph\", \"hybrid search\", \"modular prototype\", \"text\", \"image\", \"audio\", \"video\", \"pipeline logic\", \"evaluation framework\", \"queries\", \"metrics\", \"pipeline\", \"input validation\", \"query triage\", \"retrieval orchestration\", \"Answer generation\", \"multimodal RAG assistant\", \"searchable knowledge graph\", \"Vector Database\", \"ingestion pipeline\", \"domain-specific knowledge\", \"test suite\", \"Query types\", \"Evaluation goals\", \"functional unit tests\", \"Data Ingestion\", \"Preprocessing\", \"Modal-specific logic\", \"OCR/captioning\", \"Transcription\", \"Frame extraction\", \"tagging\", \"metadata\", \"domain tags\", \"Entity & Relationship Extraction\", \"LLMs\", \"Cross-modal linking\", \"schema\", \"graph database\", \"User Interface\", \"Demo\", \"Uploading new files\", \"natural language queries\", \"graph exploration\", \"Log evaluation output\", \"Scene detection\", \"Sentiment detection\", \"Topic-based reranking\", \"Real-time feedback\", \"Security-aware design\", \"Evaluation Criteria\", \"Precision\", \"relevance\", \"Latency\", \"Reliability\", \"Maintainability\", \"Resources\", \"Tips\", \"Frameworks\", \"AutoGen\", \"CrewAI\", \"DeepEval\", \"Arize Phoenix\", \"LangChain\", \"LlamaIndex\", \"Chonkie\", \"Multimodal Tools\", \"LLaVA\", \"Vector Databases\", \"Qdrant\", \"Milvus\", \"Weaviate\", \"Sample Datasets\", \"DocVQA\", \"PubLayNet\", \"LAION-400M\", \"Submission\", \"GitHub repository\", \"zip file\", \"demo video\", \"Local setup instructions\", \"evaluation report\", \"architecture decisions\"],\n",
      "  \"relationships\": [[\"Enterprise Retrieval-Augmented Generation (RAG) system\", \"supports\", \"text\"], [\"Enterprise Retrieval-Augmented Generation (RAG) system\", \"supports\", \"image\"], [\"Enterprise Retrieval-Augmented Generation (RAG) system\", \"supports\", \"audio\"], [\"Enterprise Retrieval-Augmented Generation (RAG) system\", \"supports\", \"video\"], [\"Enterprise Retrieval-Augmented Generation (RAG) system\", \"builds\", \"knowledge graph\"], [\"knowledge graph\", \"enables\", \"hybrid search\"], [\"pipeline logic\", \"defines\", \"evaluation framework\"], [\"pipeline logic\", \"structures\", \"architecture\"], [\"pipeline logic\", \"includes\", \"Input validation\"], [\"pipeline logic\", \"includes\", \"Query triage\"], [\"pipeline logic\", \"includes\", \"Agent-based retrieval orchestration\"], [\"pipeline logic\", \"includes\", \"Hybrid Search\"], [\"pipeline logic\", \"includes\", \"Answer generation\"], [\"multimodal RAG assistant\", \"ingests\", \"text\"], [\"multimodal RAG assistant\", \"ingests\", \"image\"], [\"multimodal RAG assistant\", \"ingests\", \"audio\"], [\"multimodal RAG assistant\", \"ingests\", \"video\"], [\"multimodal RAG assistant\", \"extracts\", \"entities\"], [\"multimodal RAG assistant\", \"extracts\", \"relationships\"], [\"multimodal RAG assistant\", \"constructs\", \"searchable knowledge graph\"], [\"multimodal RAG assistant\", \"builds\", \"Vector Database\"], [\"multimodal RAG assistant\", \"powers\", \"hybrid search pipeline\"], [\"Data Ingestion\", \"accepts\", \".pdf\"], [\"Data Ingestion\", \"accepts\", \".txt\"], [\"Data Ingestion\", \"accepts\", \".jpg/.png\"], [\"Data Ingestion\", \"accepts\", \".mp3/.mp4\"], [\"Data Ingestion\", \"includes\", \"Modal-specific logic\"], [\"Entity & Relationship Extraction\", \"uses\", \"LLMs\"], [\"Entity & Relationship Extraction\", \"links\", \"Cross-modal linking\"], [\"Entity & Relationship Extraction\", \"generates\", \"schema\"], [\"User Interface\", \"supports\", \"Uploading new files\"], [\"User Interface\", \"supports\", \"natural language queries\"], [\"User Interface\", \"supports\", \"graph exploration\"], [\"User Interface\", \"logs\", \"evaluation output\"], [\"Bonus Features\", \"includes\", \"Scene detection\"], [\"Bonus Features\", \"includes\", \"Sentiment detection\"], [\"Bonus Features\", \"includes\", \"Topic-based reranking\"], [\"Bonus Features\", \"includes\", \"Real-time feedback\"], [\"Bonus Features\", \"includes\", \"Security-aware design\"], [\"Evaluation Criteria\", \"evaluates\", \"Enterprise fit\"], [\"Evaluation Criteria\", \"evaluates\", \"Precision\"], [\"Evaluation Criteria\", \"evaluates\", \"Latency\"], [\"Evaluation Criteria\", \"evaluates\", \"Reliability\"], [\"Evaluation Criteria\", \"evaluates\", \"Maintainability\"], [\"Resources & Tips\", \"provides\", \"Frameworks\"], [\"Resources & Tips\", \"provides\", \"Multimodal Tools\"], [\"Resources & Tips\", \"provides\", \"Vector Databases\"], [\"Resources & Tips\", \"provides\", \"Sample Datasets\"], [\"Submission\", \"requires\", \"GitHub repository\"], [\"Submission\", \"requires\", \"zip file\"], [\"Submission\", \"requires\", \"demo video\"], [\"Submission\", \"requires\", \"Local setup instructions\"], [\"Submission\", \"optional\", \"evaluation report\"], [\"Submission\", \"optional\", \"architecture decisions\"]]\n",
      "}\n",
      "Parsing extracted JSON data...\n",
      "Inserting entities and relationships into Neo4j...\n",
      "Running similarity search on Qdrant...\n",
      "Top vector result: id='660ede96-278b-40e1-aa28-25c62e89a8cd' version=19 score=1.0 payload={'text': ' \\n72-Hour Technical Challenge \\nMultimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \\n \\nObjective \\nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \\nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \\nsearchable knowledge graph, and enables hybrid search using, keyword and vector \\nretrieval (Graph RAG System). \\nYou are expected to begin with evals to define success criteria and to structure your \\narchitecture accordingly. \\n \\nEnterprise Approach Highlights \\nBefore building any ingestion or pipeline logic, define your evaluation framework: \\n• \\nWhat constitutes a \"correct\" response? \\n• \\nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \\n• \\nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \\n• \\nHow will you fail gracefully? \\nStructure your work around a modular and scalable pipeline, including: \\n• \\nInput validation \\n• \\nQuery triage and rewriting \\n• \\nAgent-based retrieval orchestration \\n• \\nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \\nVector Retrieval \\n• \\nAnswer generation and post-processing \\n \\nChallenge Scope \\nBuild a multimodal RAG assistant that: \\n• \\nIngests at least three of the following modalities: text, image, audio, video \\n• \\nExtracts entities and relationships \\n• \\nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \\n• \\nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \\nsophisticated ingestion pipeline \\n• \\nPowers a hybrid search pipeline for fast and reliable access to domain-specific \\nknowledge \\n \\nRequirements \\n1. Evaluation-First Pipeline Design \\n• \\nDefine a minimal test suite using DeepEval or similar \\n• \\nClearly document: \\no Query types (lookup, summarization, semantic linkages) \\no Evaluation goals: retrieval quality, hallucination control, latency \\n• \\nInclude functional unit tests for each module \\n2. Data Ingestion and Preprocessing \\n• \\nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \\n• \\nModal-specific logic: \\no OCR/captioning \\no Transcription \\no Frame extraction and tagging for video \\n• \\nEnrich all outputs with metadata and domain tags \\n3. Entity & Relationship Extraction \\n• \\nUse LLMs to extract structured information \\n• \\nCross-modal linking of the same entity (e.g., “John Smith” in PDF + transcript) \\n• \\nGenerate or infer schema for graph database \\n6. User Interface / Demo \\n• \\nUI or notebook should support: \\no Uploading new files \\no Typing natural language queries \\no Viewing answers with optional graph exploration \\n• \\nLog evaluation output for each query \\n \\nBonus Features \\n• \\nScene detection for video \\n• \\nSentiment detection from text/audio \\n• \\nTopic-based reranking of results \\n• \\nReal-time feedback for query improvement \\n• \\nSecurity-aware design (query restrictions, access control) \\n \\nEvaluation Criteria \\n• \\nEnterprise fit: eval-first mindset, modularity, and clear architecture \\n• \\nPrecision and relevance: does the system retrieve the right context across \\nmodalities? \\n• \\nLatency: fast, efficient retrieval and generation \\n• \\nReliability: graceful failure handling and consistent outputs \\n• \\nMaintainability: clear logic, good documentation, testing \\nResources & Tips \\nThere’s no strict requirement for the type of dataset or topic. Suggested tools and sample \\nsets: \\nFrameworks \\n• \\nAutoGen: https://github.com/microsoft/autogen \\n• \\nCrewAI: https://github.com/joaomdmoura/crewAI \\n• \\nDeepEval: https://github.com/confident-ai/deepeval \\n• \\nArize Phoenix: https://github.com/Arize-ai/phoenix \\n• \\nLangChain: https://www.langchain.com/ \\n• \\nLlamaIndex: https://www.llamaindex.ai/ \\n• \\nChonkie: https://github.com/trychonkie/chonkie \\nMultimodal Tools \\n• \\nLLaVA: https://llava-vl.github.io/ \\nVector Databases \\n• \\nQdrant, Milvus, Weaviate \\nSample Datasets \\n• \\nDocVQA: https://docvqa.github.io/ \\n• \\nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \\n• \\nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \\n \\nSubmission \\nWithin 72 hours, submit: \\n• \\nGitHub repository or zip file \\n• \\nShort demo video (3–10 minutes) \\n• \\nLocal setup instructions \\n• \\nOptional: your evaluation report and reflections on architecture decisions \\n'} vector=None shard_key=None order_value=None\n",
      "Executing hybrid retrieval for: pdf test\n",
      "Running vector search...\n",
      "Running graph search...\n",
      "Vector Results: [' \\n72-Hour Technical Challenge \\nMultimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \\n \\nObjective \\nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \\nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \\nsearchable knowledge graph, and enables hybrid search using, keyword and vector \\nretrieval (Graph RAG System). \\nYou are expected to begin with evals to define success criteria and to structure your \\narchitecture accordingly. \\n \\nEnterprise Approach Highlights \\nBefore building any ingestion or pipeline logic, define your evaluation framework: \\n• \\nWhat constitutes a \"correct\" response? \\n• \\nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \\n• \\nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \\n• \\nHow will you fail gracefully? \\nStructure your work around a modular and scalable pipeline, including: \\n• \\nInput validation \\n• \\nQuery triage and rewriting \\n• \\nAgent-based retrieval orchestration \\n• \\nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \\nVector Retrieval \\n• \\nAnswer generation and post-processing \\n \\nChallenge Scope \\nBuild a multimodal RAG assistant that: \\n• \\nIngests at least three of the following modalities: text, image, audio, video \\n• \\nExtracts entities and relationships \\n• \\nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \\n• \\nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \\nsophisticated ingestion pipeline \\n• \\nPowers a hybrid search pipeline for fast and reliable access to domain-specific \\nknowledge \\n \\nRequirements \\n1. Evaluation-First Pipeline Design \\n• \\nDefine a minimal test suite using DeepEval or similar \\n• \\nClearly document: \\no Query types (lookup, summarization, semantic linkages) \\no Evaluation goals: retrieval quality, hallucination control, latency \\n• \\nInclude functional unit tests for each module \\n2. Data Ingestion and Preprocessing \\n• \\nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \\n• \\nModal-specific logic: \\no OCR/captioning \\no Transcription \\no Frame extraction and tagging for video \\n• \\nEnrich all outputs with metadata and domain tags \\n3. Entity & Relationship Extraction \\n• \\nUse LLMs to extract structured information \\n• \\nCross-modal linking of the same entity (e.g., “John Smith” in PDF + transcript) \\n• \\nGenerate or infer schema for graph database \\n6. User Interface / Demo \\n• \\nUI or notebook should support: \\no Uploading new files \\no Typing natural language queries \\no Viewing answers with optional graph exploration \\n• \\nLog evaluation output for each query \\n \\nBonus Features \\n• \\nScene detection for video \\n• \\nSentiment detection from text/audio \\n• \\nTopic-based reranking of results \\n• \\nReal-time feedback for query improvement \\n• \\nSecurity-aware design (query restrictions, access control) \\n \\nEvaluation Criteria \\n• \\nEnterprise fit: eval-first mindset, modularity, and clear architecture \\n• \\nPrecision and relevance: does the system retrieve the right context across \\nmodalities? \\n• \\nLatency: fast, efficient retrieval and generation \\n• \\nReliability: graceful failure handling and consistent outputs \\n• \\nMaintainability: clear logic, good documentation, testing \\nResources & Tips \\nThere’s no strict requirement for the type of dataset or topic. Suggested tools and sample \\nsets: \\nFrameworks \\n• \\nAutoGen: https://github.com/microsoft/autogen \\n• \\nCrewAI: https://github.com/joaomdmoura/crewAI \\n• \\nDeepEval: https://github.com/confident-ai/deepeval \\n• \\nArize Phoenix: https://github.com/Arize-ai/phoenix \\n• \\nLangChain: https://www.langchain.com/ \\n• \\nLlamaIndex: https://www.llamaindex.ai/ \\n• \\nChonkie: https://github.com/trychonkie/chonkie \\nMultimodal Tools \\n• \\nLLaVA: https://llava-vl.github.io/ \\nVector Databases \\n• \\nQdrant, Milvus, Weaviate \\nSample Datasets \\n• \\nDocVQA: https://docvqa.github.io/ \\n• \\nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \\n• \\nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \\n \\nSubmission \\nWithin 72 hours, submit: \\n• \\nGitHub repository or zip file \\n• \\nShort demo video (3–10 minutes) \\n• \\nLocal setup instructions \\n• \\nOptional: your evaluation report and reflections on architecture decisions \\n', ' \\n72-Hour Technical Challenge \\nMultimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \\n \\nObjective \\nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \\nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \\nsearchable knowledge graph, and enables hybrid search using, keyword and vector \\nretrieval (Graph RAG System). \\nYou are expected to begin with evals to define success criteria and to structure your \\narchitecture accordingly. \\n \\nEnterprise Approach Highlights \\nBefore building any ingestion or pipeline logic, define your evaluation framework: \\n• \\nWhat constitutes a \"correct\" response? \\n• \\nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \\n• \\nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \\n• \\nHow will you fail gracefully? \\nStructure your work around a modular and scalable pipeline, including: \\n• \\nInput validation \\n• \\nQuery triage and rewriting \\n• \\nAgent-based retrieval orchestration \\n• \\nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \\nVector Retrieval \\n• \\nAnswer generation and post-processing \\n \\nChallenge Scope \\nBuild a multimodal RAG assistant that: \\n• \\nIngests at least three of the following modalities: text, image, audio, video \\n• \\nExtracts entities and relationships \\n• \\nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \\n• \\nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \\nsophisticated ingestion pipeline \\n• \\nPowers a hybrid search pipeline for fast and reliable access to domain-specific \\nknowledge \\n \\nRequirements \\n1. Evaluation-First Pipeline Design \\n• \\nDefine a minimal test suite using DeepEval or similar \\n• \\nClearly document: \\no Query types (lookup, summarization, semantic linkages) \\no Evaluation goals: retrieval quality, hallucination control, latency \\n• \\nInclude functional unit tests for each module \\n2. Data Ingestion and Preprocessing \\n• \\nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \\n• \\nModal-specific logic: \\no OCR/captioning \\no Transcription \\no Frame extraction and tagging for video \\n• \\nEnrich all outputs with metadata and domain tags \\n3. Entity & Relationship Extraction \\n• \\nUse LLMs to extract structured information \\n• \\nCross-modal linking of the same entity (e.g., “John Smith” in PDF + transcript) \\n• \\nGenerate or infer schema for graph database \\n6. User Interface / Demo \\n• \\nUI or notebook should support: \\no Uploading new files \\no Typing natural language queries \\no Viewing answers with optional graph exploration \\n• \\nLog evaluation output for each query \\n \\nBonus Features \\n• \\nScene detection for video \\n• \\nSentiment detection from text/audio \\n• \\nTopic-based reranking of results \\n• \\nReal-time feedback for query improvement \\n• \\nSecurity-aware design (query restrictions, access control) \\n \\nEvaluation Criteria \\n• \\nEnterprise fit: eval-first mindset, modularity, and clear architecture \\n• \\nPrecision and relevance: does the system retrieve the right context across \\nmodalities? \\n• \\nLatency: fast, efficient retrieval and generation \\n• \\nReliability: graceful failure handling and consistent outputs \\n• \\nMaintainability: clear logic, good documentation, testing \\nResources & Tips \\nThere’s no strict requirement for the type of dataset or topic. Suggested tools and sample \\nsets: \\nFrameworks \\n• \\nAutoGen: https://github.com/microsoft/autogen \\n• \\nCrewAI: https://github.com/joaomdmoura/crewAI \\n• \\nDeepEval: https://github.com/confident-ai/deepeval \\n• \\nArize Phoenix: https://github.com/Arize-ai/phoenix \\n• \\nLangChain: https://www.langchain.com/ \\n• \\nLlamaIndex: https://www.llamaindex.ai/ \\n• \\nChonkie: https://github.com/trychonkie/chonkie \\nMultimodal Tools \\n• \\nLLaVA: https://llava-vl.github.io/ \\nVector Databases \\n• \\nQdrant, Milvus, Weaviate \\nSample Datasets \\n• \\nDocVQA: https://docvqa.github.io/ \\n• \\nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \\n• \\nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \\n \\nSubmission \\nWithin 72 hours, submit: \\n• \\nGitHub repository or zip file \\n• \\nShort demo video (3–10 minutes) \\n• \\nLocal setup instructions \\n• \\nOptional: your evaluation report and reflections on architecture decisions \\n', ' \\n72-Hour Technical Challenge \\nMultimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \\n \\nObjective \\nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \\nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \\nsearchable knowledge graph, and enables hybrid search using, keyword and vector \\nretrieval (Graph RAG System). \\nYou are expected to begin with evals to define success criteria and to structure your \\narchitecture accordingly. \\n \\nEnterprise Approach Highlights \\nBefore building any ingestion or pipeline logic, define your evaluation framework: \\n• \\nWhat constitutes a \"correct\" response? \\n• \\nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \\n• \\nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \\n• \\nHow will you fail gracefully? \\nStructure your work around a modular and scalable pipeline, including: \\n• \\nInput validation \\n• \\nQuery triage and rewriting \\n• \\nAgent-based retrieval orchestration \\n• \\nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \\nVector Retrieval \\n• \\nAnswer generation and post-processing \\n \\nChallenge Scope \\nBuild a multimodal RAG assistant that: \\n• \\nIngests at least three of the following modalities: text, image, audio, video \\n• \\nExtracts entities and relationships \\n• \\nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \\n• \\nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \\nsophisticated ingestion pipeline \\n• \\nPowers a hybrid search pipeline for fast and reliable access to domain-specific \\nknowledge \\n \\nRequirements \\n1. Evaluation-First Pipeline Design \\n• \\nDefine a minimal test suite using DeepEval or similar \\n• \\nClearly document: \\no Query types (lookup, summarization, semantic linkages) \\no Evaluation goals: retrieval quality, hallucination control, latency \\n• \\nInclude functional unit tests for each module \\n2. Data Ingestion and Preprocessing \\n• \\nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \\n• \\nModal-specific logic: \\no OCR/captioning \\no Transcription \\no Frame extraction and tagging for video \\n• \\nEnrich all outputs with metadata and domain tags \\n3. Entity & Relationship Extraction \\n• \\nUse LLMs to extract structured information \\n• \\nCross-modal linking of the same entity (e.g., “John Smith” in PDF + transcript) \\n• \\nGenerate or infer schema for graph database \\n6. User Interface / Demo \\n• \\nUI or notebook should support: \\no Uploading new files \\no Typing natural language queries \\no Viewing answers with optional graph exploration \\n• \\nLog evaluation output for each query \\n \\nBonus Features \\n• \\nScene detection for video \\n• \\nSentiment detection from text/audio \\n• \\nTopic-based reranking of results \\n• \\nReal-time feedback for query improvement \\n• \\nSecurity-aware design (query restrictions, access control) \\n \\nEvaluation Criteria \\n• \\nEnterprise fit: eval-first mindset, modularity, and clear architecture \\n• \\nPrecision and relevance: does the system retrieve the right context across \\nmodalities? \\n• \\nLatency: fast, efficient retrieval and generation \\n• \\nReliability: graceful failure handling and consistent outputs \\n• \\nMaintainability: clear logic, good documentation, testing \\nResources & Tips \\nThere’s no strict requirement for the type of dataset or topic. Suggested tools and sample \\nsets: \\nFrameworks \\n• \\nAutoGen: https://github.com/microsoft/autogen \\n• \\nCrewAI: https://github.com/joaomdmoura/crewAI \\n• \\nDeepEval: https://github.com/confident-ai/deepeval \\n• \\nArize Phoenix: https://github.com/Arize-ai/phoenix \\n• \\nLangChain: https://www.langchain.com/ \\n• \\nLlamaIndex: https://www.llamaindex.ai/ \\n• \\nChonkie: https://github.com/trychonkie/chonkie \\nMultimodal Tools \\n• \\nLLaVA: https://llava-vl.github.io/ \\nVector Databases \\n• \\nQdrant, Milvus, Weaviate \\nSample Datasets \\n• \\nDocVQA: https://docvqa.github.io/ \\n• \\nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \\n• \\nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \\n \\nSubmission \\nWithin 72 hours, submit: \\n• \\nGitHub repository or zip file \\n• \\nShort demo video (3–10 minutes) \\n• \\nLocal setup instructions \\n• \\nOptional: your evaluation report and reflections on architecture decisions \\n']\n",
      "Graph Results: []\n",
      "Pipeline execution complete. Results saved to results/hybrid_results.json\n"
     ]
    }
   ],
   "source": [
    "!python main.py assets/sample_pdf.pdf \"pdf test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf54055-32af-4ddc-921c-c047d3fc71b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from: assets/sample_audio.mp3\n",
      "Text extracted:  It's a weekend. In John decided to watch the latest movie recommended by Netflix at his friend's place. Before heading out, he asked Siri about the weather and realized it would rain. So he decided to take his Tesla for the long journey and switch to autopilot on the highway. After coming home from the eventful day, he started wondering how technology has made his life easy. He did some research on the internet and found out that Netflix, Siri and Tesla are all using AI. So what is AI? AI or ar ...\n",
      "Initializing Qdrant and inserting vector...\n",
      "Running entity and relationship extraction...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "LLM response:\n",
      "{\n",
      "  \"entities\": [\"John\", \"Netflix\", \"Siri\", \"Tesla\", \"AI\", \"John McCarthy\", \"IBM's Watson\", \"GPT-3\", \"machine learning\", \"deep learning\", \"natural language processing\"],\n",
      "  \"relationships\": [\n",
      "    [\"John\", \"watched\", \"Netflix\"],\n",
      "    [\"John\", \"asked\", \"Siri\"],\n",
      "    [\"John\", \"used\", \"Tesla\"],\n",
      "    [\"Netflix\", \"recommends\", \"movie\"],\n",
      "    [\"Siri\", \"provides\", \"weather information\"],\n",
      "    [\"Tesla\", \"has\", \"autopilot feature\"],\n",
      "    [\"AI\", \"used by\", \"Netflix\"],\n",
      "    [\"AI\", \"used by\", \"Siri\"],\n",
      "    [\"AI\", \"used by\", \"Tesla\"],\n",
      "    [\"AI\", \"coined by\", \"John McCarthy\"],\n",
      "    [\"AI\", \"classified into\", \"artificial narrow intelligence\"],\n",
      "    [\"AI\", \"classified into\", \"artificial general intelligence\"],\n",
      "    [\"AI\", \"classified into\", \"artificial super intelligence\"],\n",
      "    [\"artificial narrow intelligence\", \"examples include\", \"Netflix\"],\n",
      "    [\"artificial narrow intelligence\", \"examples include\", \"Siri\"],\n",
      "    [\"artificial general intelligence\", \"examples include\", \"IBM's Watson\"],\n",
      "    [\"artificial general intelligence\", \"examples include\", \"GPT-3\"],\n",
      "    [\"machine learning\", \"subset of\", \"AI\"],\n",
      "    [\"machine learning\", \"used in\", \"email spam detection\"],\n",
      "    [\"machine learning\", \"used in\", \"medical diagnosis\"],\n",
      "    [\"deep learning\", \"subset of\", \"machine learning\"],\n",
      "    [\"deep learning\", \"works with\", \"artificial neural networks\"],\n",
      "    [\"deep learning\", \"used in\", \"face recognition\"],\n",
      "    [\"deep learning\", \"used in\", \"speech recognition\"],\n",
      "    [\"natural language processing\", \"subset of\", \"AI\"],\n",
      "    [\"natural language processing\", \"used in\", \"NLP\"],\n",
      "    [\"natural language processing\", \"used in\", \"chatbots\"]\n",
      "  ]\n",
      "}\n",
      "Parsing extracted JSON data...\n",
      "Inserting entities and relationships into Neo4j...\n",
      "Running similarity search on Qdrant...\n",
      "Top vector result: id='7deea2ec-29a8-4849-9107-4bae474bd513' version=3 score=1.0 payload={'text': \" It's a weekend. In John decided to watch the latest movie recommended by Netflix at his friend's place. Before heading out, he asked Siri about the weather and realized it would rain. So he decided to take his Tesla for the long journey and switch to autopilot on the highway. After coming home from the eventful day, he started wondering how technology has made his life easy. He did some research on the internet and found out that Netflix, Siri and Tesla are all using AI. So what is AI? AI or artificial intelligence is nothing but making computers-based machines think and act like humans. Artificial intelligence is not a new term. John McCarthy, a computer scientist, coined the term artificial intelligence back in 1956. But it took time to evolve as it demanded heavy computing power. Artificial intelligence is not confined to just movie recommendations and virtual assistance. Broadly classifying, there are three types of AI. Artificial narrow intelligence, also called weak AI, is the stage where machines can perform a specific task. Netflix, Siri, chatbots, spatial recommendation systems are all examples of artificial narrow intelligence. Next up, we have artificial general intelligence referred to as an intelligent agent's capacity to comprehend or pick up any intellectual skill that a human can. We are halfway into successfully implementing this space. IBM's Watson's supercomputer in GPT-3 fall under this category. And lastly, artificial super intelligence. It is the stage where machines surpass human intelligence. You might have seen this in movies and imagined how the world would be if machines occupied. Fascinated by this, John did more research and found out that machine learning, deep learning and natural language processing are all connected with artificial intelligence. Machine learning, a subset of AI, is the process of automating and enhancing how computers learn from their experiences without human health. Machine learning can be used in email spam detection, medical diagnosis, etc. Deep learning can be considered a subset of machine learning. It is a field that is based on learning and improving on its own by examining computer algorithms. While machine learning uses simpler concepts, deep learning works with artificial neural networks, which are designed to imitate the human right. This technology can be applied in face recognition, speech recognition, and many more applications. Natural language processing, popularly known as NLP, can be defined as the ability of machines to learn human language and translate it. Chatbots fall under this category. Natural intelligence is advancing in every crucial field like health care, education, robotics, banking, e-commerce, and the list goes on. Like in health care, AI is used to identify diseases. Helping health care service providers and their patients make better treatment and lifestyle decisions. Coming to the education sector, AI is helping teachers automate grading, organizing and facilitating parent guardian conversations. In robotics, AI-powered robots employ real-time updates to detect obstructions in their path, and instantaneously design their routes. Artificial intelligence provides advanced data analytics that is transforming banking by reducing fraud and enhancing compliance. With this growing demand for AI, more and more industries are looking for AI engineers who can help them develop intelligent systems and offer them lucrative salaries going north of $120,000. The future of AI looks promising with the AI market expected to reach $190 billion by 2025. So on that note, I have a question for you. Artificial intelligence is about playing a computer game, creating a device using your own intelligence, to program an intelligent machine, investing your brain power in a machine, give the correct answer along with your reasoning, and stand a chance to win an Amazon voucher. Think about it, and leave your answers in the comments section, and we will provide the answer next week. We hope you enjoyed this video. If you did, a thumbs up would be really appreciated. Here's your reminder to subscribe to our channel, and click on the bell icon for more on the latest technologies and trends. Thank you for watching, and stay tuned for more from Simply Learn.\"} vector=None shard_key=None order_value=None\n",
      "Executing hybrid retrieval for: mp3 test\n",
      "Running vector search...\n",
      "Running graph search...\n",
      "Vector Results: [' \\n72-Hour Technical Challenge \\nMultimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \\n \\nObjective \\nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \\nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \\nsearchable knowledge graph, and enables hybrid search using, keyword and vector \\nretrieval (Graph RAG System). \\nYou are expected to begin with evals to define success criteria and to structure your \\narchitecture accordingly. \\n \\nEnterprise Approach Highlights \\nBefore building any ingestion or pipeline logic, define your evaluation framework: \\n• \\nWhat constitutes a \"correct\" response? \\n• \\nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \\n• \\nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \\n• \\nHow will you fail gracefully? \\nStructure your work around a modular and scalable pipeline, including: \\n• \\nInput validation \\n• \\nQuery triage and rewriting \\n• \\nAgent-based retrieval orchestration \\n• \\nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \\nVector Retrieval \\n• \\nAnswer generation and post-processing \\n \\nChallenge Scope \\nBuild a multimodal RAG assistant that: \\n• \\nIngests at least three of the following modalities: text, image, audio, video \\n• \\nExtracts entities and relationships \\n• \\nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \\n• \\nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \\nsophisticated ingestion pipeline \\n• \\nPowers a hybrid search pipeline for fast and reliable access to domain-specific \\nknowledge \\n \\nRequirements \\n1. Evaluation-First Pipeline Design \\n• \\nDefine a minimal test suite using DeepEval or similar \\n• \\nClearly document: \\no Query types (lookup, summarization, semantic linkages) \\no Evaluation goals: retrieval quality, hallucination control, latency \\n• \\nInclude functional unit tests for each module \\n2. Data Ingestion and Preprocessing \\n• \\nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \\n• \\nModal-specific logic: \\no OCR/captioning \\no Transcription \\no Frame extraction and tagging for video \\n• \\nEnrich all outputs with metadata and domain tags \\n3. Entity & Relationship Extraction \\n• \\nUse LLMs to extract structured information \\n• \\nCross-modal linking of the same entity (e.g., “John Smith” in PDF + transcript) \\n• \\nGenerate or infer schema for graph database \\n6. User Interface / Demo \\n• \\nUI or notebook should support: \\no Uploading new files \\no Typing natural language queries \\no Viewing answers with optional graph exploration \\n• \\nLog evaluation output for each query \\n \\nBonus Features \\n• \\nScene detection for video \\n• \\nSentiment detection from text/audio \\n• \\nTopic-based reranking of results \\n• \\nReal-time feedback for query improvement \\n• \\nSecurity-aware design (query restrictions, access control) \\n \\nEvaluation Criteria \\n• \\nEnterprise fit: eval-first mindset, modularity, and clear architecture \\n• \\nPrecision and relevance: does the system retrieve the right context across \\nmodalities? \\n• \\nLatency: fast, efficient retrieval and generation \\n• \\nReliability: graceful failure handling and consistent outputs \\n• \\nMaintainability: clear logic, good documentation, testing \\nResources & Tips \\nThere’s no strict requirement for the type of dataset or topic. Suggested tools and sample \\nsets: \\nFrameworks \\n• \\nAutoGen: https://github.com/microsoft/autogen \\n• \\nCrewAI: https://github.com/joaomdmoura/crewAI \\n• \\nDeepEval: https://github.com/confident-ai/deepeval \\n• \\nArize Phoenix: https://github.com/Arize-ai/phoenix \\n• \\nLangChain: https://www.langchain.com/ \\n• \\nLlamaIndex: https://www.llamaindex.ai/ \\n• \\nChonkie: https://github.com/trychonkie/chonkie \\nMultimodal Tools \\n• \\nLLaVA: https://llava-vl.github.io/ \\nVector Databases \\n• \\nQdrant, Milvus, Weaviate \\nSample Datasets \\n• \\nDocVQA: https://docvqa.github.io/ \\n• \\nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \\n• \\nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \\n \\nSubmission \\nWithin 72 hours, submit: \\n• \\nGitHub repository or zip file \\n• \\nShort demo video (3–10 minutes) \\n• \\nLocal setup instructions \\n• \\nOptional: your evaluation report and reflections on architecture decisions \\n', ' \\n72-Hour Technical Challenge \\nMultimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \\n \\nObjective \\nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \\nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \\nsearchable knowledge graph, and enables hybrid search using, keyword and vector \\nretrieval (Graph RAG System). \\nYou are expected to begin with evals to define success criteria and to structure your \\narchitecture accordingly. \\n \\nEnterprise Approach Highlights \\nBefore building any ingestion or pipeline logic, define your evaluation framework: \\n• \\nWhat constitutes a \"correct\" response? \\n• \\nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \\n• \\nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \\n• \\nHow will you fail gracefully? \\nStructure your work around a modular and scalable pipeline, including: \\n• \\nInput validation \\n• \\nQuery triage and rewriting \\n• \\nAgent-based retrieval orchestration \\n• \\nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \\nVector Retrieval \\n• \\nAnswer generation and post-processing \\n \\nChallenge Scope \\nBuild a multimodal RAG assistant that: \\n• \\nIngests at least three of the following modalities: text, image, audio, video \\n• \\nExtracts entities and relationships \\n• \\nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \\n• \\nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \\nsophisticated ingestion pipeline \\n• \\nPowers a hybrid search pipeline for fast and reliable access to domain-specific \\nknowledge \\n \\nRequirements \\n1. Evaluation-First Pipeline Design \\n• \\nDefine a minimal test suite using DeepEval or similar \\n• \\nClearly document: \\no Query types (lookup, summarization, semantic linkages) \\no Evaluation goals: retrieval quality, hallucination control, latency \\n• \\nInclude functional unit tests for each module \\n2. Data Ingestion and Preprocessing \\n• \\nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \\n• \\nModal-specific logic: \\no OCR/captioning \\no Transcription \\no Frame extraction and tagging for video \\n• \\nEnrich all outputs with metadata and domain tags \\n3. Entity & Relationship Extraction \\n• \\nUse LLMs to extract structured information \\n• \\nCross-modal linking of the same entity (e.g., “John Smith” in PDF + transcript) \\n• \\nGenerate or infer schema for graph database \\n6. User Interface / Demo \\n• \\nUI or notebook should support: \\no Uploading new files \\no Typing natural language queries \\no Viewing answers with optional graph exploration \\n• \\nLog evaluation output for each query \\n \\nBonus Features \\n• \\nScene detection for video \\n• \\nSentiment detection from text/audio \\n• \\nTopic-based reranking of results \\n• \\nReal-time feedback for query improvement \\n• \\nSecurity-aware design (query restrictions, access control) \\n \\nEvaluation Criteria \\n• \\nEnterprise fit: eval-first mindset, modularity, and clear architecture \\n• \\nPrecision and relevance: does the system retrieve the right context across \\nmodalities? \\n• \\nLatency: fast, efficient retrieval and generation \\n• \\nReliability: graceful failure handling and consistent outputs \\n• \\nMaintainability: clear logic, good documentation, testing \\nResources & Tips \\nThere’s no strict requirement for the type of dataset or topic. Suggested tools and sample \\nsets: \\nFrameworks \\n• \\nAutoGen: https://github.com/microsoft/autogen \\n• \\nCrewAI: https://github.com/joaomdmoura/crewAI \\n• \\nDeepEval: https://github.com/confident-ai/deepeval \\n• \\nArize Phoenix: https://github.com/Arize-ai/phoenix \\n• \\nLangChain: https://www.langchain.com/ \\n• \\nLlamaIndex: https://www.llamaindex.ai/ \\n• \\nChonkie: https://github.com/trychonkie/chonkie \\nMultimodal Tools \\n• \\nLLaVA: https://llava-vl.github.io/ \\nVector Databases \\n• \\nQdrant, Milvus, Weaviate \\nSample Datasets \\n• \\nDocVQA: https://docvqa.github.io/ \\n• \\nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \\n• \\nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \\n \\nSubmission \\nWithin 72 hours, submit: \\n• \\nGitHub repository or zip file \\n• \\nShort demo video (3–10 minutes) \\n• \\nLocal setup instructions \\n• \\nOptional: your evaluation report and reflections on architecture decisions \\n', ' \\n72-Hour Technical Challenge \\nMultimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \\n \\nObjective \\nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \\nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \\nsearchable knowledge graph, and enables hybrid search using, keyword and vector \\nretrieval (Graph RAG System). \\nYou are expected to begin with evals to define success criteria and to structure your \\narchitecture accordingly. \\n \\nEnterprise Approach Highlights \\nBefore building any ingestion or pipeline logic, define your evaluation framework: \\n• \\nWhat constitutes a \"correct\" response? \\n• \\nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \\n• \\nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \\n• \\nHow will you fail gracefully? \\nStructure your work around a modular and scalable pipeline, including: \\n• \\nInput validation \\n• \\nQuery triage and rewriting \\n• \\nAgent-based retrieval orchestration \\n• \\nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \\nVector Retrieval \\n• \\nAnswer generation and post-processing \\n \\nChallenge Scope \\nBuild a multimodal RAG assistant that: \\n• \\nIngests at least three of the following modalities: text, image, audio, video \\n• \\nExtracts entities and relationships \\n• \\nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \\n• \\nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \\nsophisticated ingestion pipeline \\n• \\nPowers a hybrid search pipeline for fast and reliable access to domain-specific \\nknowledge \\n \\nRequirements \\n1. Evaluation-First Pipeline Design \\n• \\nDefine a minimal test suite using DeepEval or similar \\n• \\nClearly document: \\no Query types (lookup, summarization, semantic linkages) \\no Evaluation goals: retrieval quality, hallucination control, latency \\n• \\nInclude functional unit tests for each module \\n2. Data Ingestion and Preprocessing \\n• \\nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \\n• \\nModal-specific logic: \\no OCR/captioning \\no Transcription \\no Frame extraction and tagging for video \\n• \\nEnrich all outputs with metadata and domain tags \\n3. Entity & Relationship Extraction \\n• \\nUse LLMs to extract structured information \\n• \\nCross-modal linking of the same entity (e.g., “John Smith” in PDF + transcript) \\n• \\nGenerate or infer schema for graph database \\n6. User Interface / Demo \\n• \\nUI or notebook should support: \\no Uploading new files \\no Typing natural language queries \\no Viewing answers with optional graph exploration \\n• \\nLog evaluation output for each query \\n \\nBonus Features \\n• \\nScene detection for video \\n• \\nSentiment detection from text/audio \\n• \\nTopic-based reranking of results \\n• \\nReal-time feedback for query improvement \\n• \\nSecurity-aware design (query restrictions, access control) \\n \\nEvaluation Criteria \\n• \\nEnterprise fit: eval-first mindset, modularity, and clear architecture \\n• \\nPrecision and relevance: does the system retrieve the right context across \\nmodalities? \\n• \\nLatency: fast, efficient retrieval and generation \\n• \\nReliability: graceful failure handling and consistent outputs \\n• \\nMaintainability: clear logic, good documentation, testing \\nResources & Tips \\nThere’s no strict requirement for the type of dataset or topic. Suggested tools and sample \\nsets: \\nFrameworks \\n• \\nAutoGen: https://github.com/microsoft/autogen \\n• \\nCrewAI: https://github.com/joaomdmoura/crewAI \\n• \\nDeepEval: https://github.com/confident-ai/deepeval \\n• \\nArize Phoenix: https://github.com/Arize-ai/phoenix \\n• \\nLangChain: https://www.langchain.com/ \\n• \\nLlamaIndex: https://www.llamaindex.ai/ \\n• \\nChonkie: https://github.com/trychonkie/chonkie \\nMultimodal Tools \\n• \\nLLaVA: https://llava-vl.github.io/ \\nVector Databases \\n• \\nQdrant, Milvus, Weaviate \\nSample Datasets \\n• \\nDocVQA: https://docvqa.github.io/ \\n• \\nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \\n• \\nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \\n \\nSubmission \\nWithin 72 hours, submit: \\n• \\nGitHub repository or zip file \\n• \\nShort demo video (3–10 minutes) \\n• \\nLocal setup instructions \\n• \\nOptional: your evaluation report and reflections on architecture decisions \\n']\n",
      "Graph Results: []\n",
      "Pipeline execution complete. Results saved to results/hybrid_results.json\n"
     ]
    }
   ],
   "source": [
    "!python main.py assets/sample_audio.mp3 \"mp3 test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45781b93-ee03-46d0-a5e1-625dda1977ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from: assets/Aiinfo.mp4\n",
      "MoviePy - Writing audio in temp_audio.wav\n",
      "MoviePy - Done.                                                                 \n",
      "Text extracted:  Everybody's talking about artificial intelligence these days, AI. Machine learning is another hot topic. Are they the same thing or are they different? And if so, what are those differences? And deep learning is another one that comes into play. I actually did a video on these three, artificial intelligence, machine learning and deep learning, and talked about where they fit. And there were a lot of comments on that, and I read those comments, and I'd like to address some of the most frequently ...\n",
      "Initializing Qdrant and inserting vector...\n",
      "Running entity and relationship extraction...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "LLM response:\n",
      "{\n",
      "  \"entities\": [\"artificial intelligence\", \"machine learning\", \"deep learning\", \"generative AI\", \"large language models\", \"chatbots\", \"deep fakes\", \"neural networks\"],\n",
      "  \"relationships\": [\n",
      "    [\"artificial intelligence\", \"is\", \"trying to simulate with a computer something that would match or exceed human intelligence\"],\n",
      "    [\"machine learning\", \"is\", \"the machine is learning\"],\n",
      "    [\"deep learning\", \"involves\", \"neural networks\"],\n",
      "    [\"generative AI\", \"involves\", \"foundation models\"],\n",
      "    [\"foundation models\", \"include\", \"large language models\"],\n",
      "    [\"foundation models\", \"include\", \"chatbots\"],\n",
      "    [\"foundation models\", \"include\", \"deep fakes\"],\n",
      "    [\"neural networks\", \"simulate\", \"the way the human brain works\"]\n",
      "  ]\n",
      "}\n",
      "Parsing extracted JSON data...\n",
      "Inserting entities and relationships into Neo4j...\n",
      "Running similarity search on Qdrant...\n",
      "Top vector result: id='a631ea3c-81af-496e-a766-d80d1069fc39' version=24 score=0.99999994 payload={'text': \" Everybody's talking about artificial intelligence these days, AI. Machine learning is another hot topic. Are they the same thing or are they different? And if so, what are those differences? And deep learning is another one that comes into play. I actually did a video on these three, artificial intelligence, machine learning and deep learning, and talked about where they fit. And there were a lot of comments on that, and I read those comments, and I'd like to address some of the most frequently asked questions so that we can clear up some of the myths and misconceptions around this. In addition, something else has happened since that video was recorded. And that is the absolute explosion of this area of generative AI. Things like large language models and chatbots has seemed to be taking over the world. We see them everywhere. Really interesting technology. And then also things like deep fakes. These are all within the realm of AI, but how do they fit within each other? How are they related to each other? We're going to take a look at that in this video and try to explain how all these technologies relate and how we can use them. First off, a little bit of a disclaimer. I'm going to have to simplify some of these concepts in order to not make this video last for a week. So those of you that are really deep experts in the field apologies in advance. But we're going to try to make this simple and that will involve some generalizations. First of all, to start with AI. Artificial intelligence is basically trying to simulate with a computer something that would match or exceed human intelligence. What is intelligence? Well, it could be a lot of different things, but generally we tend to think of it as the ability to learn, to infer, and to reason, things like that. So that's what we're trying to do in the broad field of AI, of artificial intelligence. And if we look at a timeline of AI, it really kind of started back around this time frame. And in those days, it was very premature. Most people had not even heard of it. And it basically was a research project. But I can tell you, as an undergrad, which for me was back during these times, we were doing AI work. In fact, we would use programming languages like Lisp or ProLog. And these kinds of things were kind of the predecessors to what became later expert systems. And this was a technology. Again, some of these things existed previous, but that's when it really hit the kind of a critical mass and became more popularized. So expert systems of the 1980s, maybe in the 90s. And again, we used technologies like this. All of this was something that we did before we ever touched in to the next topic I'm going to talk about. And that's the area of machine learning. Machine learning is, as its name implies, the machine is learning. I don't have to program it. I give it lots of information and it observes things. So for instance, if I start doing this, if I give you this and then ask you to predict what's the next thing that's going to be there, well, you might get it, you might not. You have very limited training data to base this on. But if I gave you one of those and then ask you what to predict, would happen next, well, you're probably going to say this. And then you're going to say it's this. And then you think you got it all figured out. And then you see one of these. And then all of a sudden, I give you one of those and throw you a curveball. So this, in fact, and then maybe it goes on like this. So a machine learning algorithm is really good at looking at patterns and discovering patterns within data. The more training data you can give it, the more confident it can be in predicting. So predictions are one of the things that machine learning is particularly good at. Another thing is spotting outliers like this and saying, oh, that doesn't belong in, it looks different than all the other stuff because the sequence was broken. So that's particularly useful in cybersecurity, the area that I work in, because we're looking for outliers. We're looking for users who are using the system in ways that they shouldn't be or ways that they don't typically do. So this technology machine learning is particularly useful for us. And machine learning really came along and became more popularized in this time frame, in the 2010s. And again, back when I was an undergrad riding my dinosaur to class, we were doing this kind of stuff. We never once talked about machine learning. It might have existed, but it really wasn't, hadn't hit the popular mindset yet. But this technology has matured greatly over the last few decades, and now it becomes the basis of a lot we do going forward. The next layer of our Venn diagram involves deep learning. Well, it's deep learning in the sense that with deep learning we use these things called neural networks. Neural networks are ways that in a computer we simulate and mimic the way the human brain works, at least to the extent that we understand how the brain works. And it's called deep because we have multiple layers of those neural networks. And the interesting thing about these is they will simulate the way a brain operates but I don't know if you've noticed, but human brains can be a little bit unpredictable. You put certain things in, you don't always get the very same thing out. And deep learning is the same way. In some cases, we're not actually able to fully understand why we get the results we do because there are so many layers to the neural network. It's a little bit hard to decompose and figure out exactly what's in there. But this has become a very important part and a very important advancement. That also reached some popularity during the 2010s and as something that we use still today as the basis for our next area of AI. The most recent advancements in the field of artificial intelligence all really are in this space, the area of generative AI. Now, I'm going to introduce a term that you may not be familiar with. It's the idea of foundation models. Foundation models is where we get some of these kinds of things. For instance, an example of a foundation model would be a large language model, which is where we take language and we model it. And we make predictions in this technology where if I see certain types of words, then I can sort of predict what the next set of words will be. I'm going to oversimplify here for the sake of simplicity. But think about this as a little bit like the autocomplete. When you start typing something in and then it predicts what your next word will be, except in this case with large language models, they're not predicting the next word. They're predicting the next sentence, the next paragraph, the next entire document. So there's a really an amazing exponential leap in what these things are able to do. And we call all of these technologies generative because they are generating new content. Some people have actually made the argument that the generative AI isn't really generative that these technologies are really just regurgitating existing information and putting it in different format. Well, let me give you an analogy. If you take music, for instance, then every note has already been invented. So in a sense, every song is just a recombination, some other permutation of all the notes that already exist already and just putting them in a different order. Well, we don't say new music doesn't exist. People are still composing and creating new songs from the existing information. I'm going to say gen AI is similar. It's an analogy, so there are beast and mem perfections in it, but you get the general idea. Actually, new content can be generated out of these. And there are a lot of different forms that this can take. Other types of models are audio models, video models, and things like that. Well, in fact, these we can use to create deepfakes. And deepfakes are examples where we're able to take, for instance, a person's voice and recreate that. And then have it seem like the person said things they never said. Well, it's really useful in entertainment situations, in parodies and things like that, or if someone's losing their voice, then you could capture their voice, and then they'd be able to type, and you'd be able to hear it in their voice. But there's also a lot of cases where this stuff could be abused. The chat bots, again, come from this space. The deepfakes come from this space. But they're all part of generative AI and all part of these foundation models. And this, again, is the area that has really caused all of us to really pay attention to AI. The possibilities of generating new content, or in some cases, summarizing existing content and giving us something that is bite size and manageable. This is what has gotten all of the attention. This is where the chat bots and all of these things come in. In the early days, AI's adoption started off pretty slowly. Most people didn't even know it existed. And if they did, it was something that always seemed like it was about five to 10 years away. But then machine learning, deep learning, and things like that came along, and we started seeing some uptick. Then foundation models, Gen AI, and the light came along and this stuff went straight to the moon. These foundation models are what have changed the adoption curve, and now you see AI being adopted everywhere. And the thing for us to understand is where this is, where it fits in, and make sure that we can reap the benefits from all of this technology. If you like this video and want to see more like it, please like and subscribe. If you have any questions or want to share your thoughts about this topic, please leave a comment below.\"} vector=None shard_key=None order_value=None\n",
      "Executing hybrid retrieval for: mp4 test\n",
      "Running vector search...\n",
      "Running graph search...\n",
      "Vector Results: [' \\n72-Hour Technical Challenge \\nMultimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \\n \\nObjective \\nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \\nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \\nsearchable knowledge graph, and enables hybrid search using, keyword and vector \\nretrieval (Graph RAG System). \\nYou are expected to begin with evals to define success criteria and to structure your \\narchitecture accordingly. \\n \\nEnterprise Approach Highlights \\nBefore building any ingestion or pipeline logic, define your evaluation framework: \\n• \\nWhat constitutes a \"correct\" response? \\n• \\nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \\n• \\nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \\n• \\nHow will you fail gracefully? \\nStructure your work around a modular and scalable pipeline, including: \\n• \\nInput validation \\n• \\nQuery triage and rewriting \\n• \\nAgent-based retrieval orchestration \\n• \\nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \\nVector Retrieval \\n• \\nAnswer generation and post-processing \\n \\nChallenge Scope \\nBuild a multimodal RAG assistant that: \\n• \\nIngests at least three of the following modalities: text, image, audio, video \\n• \\nExtracts entities and relationships \\n• \\nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \\n• \\nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \\nsophisticated ingestion pipeline \\n• \\nPowers a hybrid search pipeline for fast and reliable access to domain-specific \\nknowledge \\n \\nRequirements \\n1. Evaluation-First Pipeline Design \\n• \\nDefine a minimal test suite using DeepEval or similar \\n• \\nClearly document: \\no Query types (lookup, summarization, semantic linkages) \\no Evaluation goals: retrieval quality, hallucination control, latency \\n• \\nInclude functional unit tests for each module \\n2. Data Ingestion and Preprocessing \\n• \\nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \\n• \\nModal-specific logic: \\no OCR/captioning \\no Transcription \\no Frame extraction and tagging for video \\n• \\nEnrich all outputs with metadata and domain tags \\n3. Entity & Relationship Extraction \\n• \\nUse LLMs to extract structured information \\n• \\nCross-modal linking of the same entity (e.g., “John Smith” in PDF + transcript) \\n• \\nGenerate or infer schema for graph database \\n6. User Interface / Demo \\n• \\nUI or notebook should support: \\no Uploading new files \\no Typing natural language queries \\no Viewing answers with optional graph exploration \\n• \\nLog evaluation output for each query \\n \\nBonus Features \\n• \\nScene detection for video \\n• \\nSentiment detection from text/audio \\n• \\nTopic-based reranking of results \\n• \\nReal-time feedback for query improvement \\n• \\nSecurity-aware design (query restrictions, access control) \\n \\nEvaluation Criteria \\n• \\nEnterprise fit: eval-first mindset, modularity, and clear architecture \\n• \\nPrecision and relevance: does the system retrieve the right context across \\nmodalities? \\n• \\nLatency: fast, efficient retrieval and generation \\n• \\nReliability: graceful failure handling and consistent outputs \\n• \\nMaintainability: clear logic, good documentation, testing \\nResources & Tips \\nThere’s no strict requirement for the type of dataset or topic. Suggested tools and sample \\nsets: \\nFrameworks \\n• \\nAutoGen: https://github.com/microsoft/autogen \\n• \\nCrewAI: https://github.com/joaomdmoura/crewAI \\n• \\nDeepEval: https://github.com/confident-ai/deepeval \\n• \\nArize Phoenix: https://github.com/Arize-ai/phoenix \\n• \\nLangChain: https://www.langchain.com/ \\n• \\nLlamaIndex: https://www.llamaindex.ai/ \\n• \\nChonkie: https://github.com/trychonkie/chonkie \\nMultimodal Tools \\n• \\nLLaVA: https://llava-vl.github.io/ \\nVector Databases \\n• \\nQdrant, Milvus, Weaviate \\nSample Datasets \\n• \\nDocVQA: https://docvqa.github.io/ \\n• \\nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \\n• \\nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \\n \\nSubmission \\nWithin 72 hours, submit: \\n• \\nGitHub repository or zip file \\n• \\nShort demo video (3–10 minutes) \\n• \\nLocal setup instructions \\n• \\nOptional: your evaluation report and reflections on architecture decisions \\n', ' \\n72-Hour Technical Challenge \\nMultimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \\n \\nObjective \\nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \\nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \\nsearchable knowledge graph, and enables hybrid search using, keyword and vector \\nretrieval (Graph RAG System). \\nYou are expected to begin with evals to define success criteria and to structure your \\narchitecture accordingly. \\n \\nEnterprise Approach Highlights \\nBefore building any ingestion or pipeline logic, define your evaluation framework: \\n• \\nWhat constitutes a \"correct\" response? \\n• \\nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \\n• \\nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \\n• \\nHow will you fail gracefully? \\nStructure your work around a modular and scalable pipeline, including: \\n• \\nInput validation \\n• \\nQuery triage and rewriting \\n• \\nAgent-based retrieval orchestration \\n• \\nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \\nVector Retrieval \\n• \\nAnswer generation and post-processing \\n \\nChallenge Scope \\nBuild a multimodal RAG assistant that: \\n• \\nIngests at least three of the following modalities: text, image, audio, video \\n• \\nExtracts entities and relationships \\n• \\nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \\n• \\nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \\nsophisticated ingestion pipeline \\n• \\nPowers a hybrid search pipeline for fast and reliable access to domain-specific \\nknowledge \\n \\nRequirements \\n1. Evaluation-First Pipeline Design \\n• \\nDefine a minimal test suite using DeepEval or similar \\n• \\nClearly document: \\no Query types (lookup, summarization, semantic linkages) \\no Evaluation goals: retrieval quality, hallucination control, latency \\n• \\nInclude functional unit tests for each module \\n2. Data Ingestion and Preprocessing \\n• \\nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \\n• \\nModal-specific logic: \\no OCR/captioning \\no Transcription \\no Frame extraction and tagging for video \\n• \\nEnrich all outputs with metadata and domain tags \\n3. Entity & Relationship Extraction \\n• \\nUse LLMs to extract structured information \\n• \\nCross-modal linking of the same entity (e.g., “John Smith” in PDF + transcript) \\n• \\nGenerate or infer schema for graph database \\n6. User Interface / Demo \\n• \\nUI or notebook should support: \\no Uploading new files \\no Typing natural language queries \\no Viewing answers with optional graph exploration \\n• \\nLog evaluation output for each query \\n \\nBonus Features \\n• \\nScene detection for video \\n• \\nSentiment detection from text/audio \\n• \\nTopic-based reranking of results \\n• \\nReal-time feedback for query improvement \\n• \\nSecurity-aware design (query restrictions, access control) \\n \\nEvaluation Criteria \\n• \\nEnterprise fit: eval-first mindset, modularity, and clear architecture \\n• \\nPrecision and relevance: does the system retrieve the right context across \\nmodalities? \\n• \\nLatency: fast, efficient retrieval and generation \\n• \\nReliability: graceful failure handling and consistent outputs \\n• \\nMaintainability: clear logic, good documentation, testing \\nResources & Tips \\nThere’s no strict requirement for the type of dataset or topic. Suggested tools and sample \\nsets: \\nFrameworks \\n• \\nAutoGen: https://github.com/microsoft/autogen \\n• \\nCrewAI: https://github.com/joaomdmoura/crewAI \\n• \\nDeepEval: https://github.com/confident-ai/deepeval \\n• \\nArize Phoenix: https://github.com/Arize-ai/phoenix \\n• \\nLangChain: https://www.langchain.com/ \\n• \\nLlamaIndex: https://www.llamaindex.ai/ \\n• \\nChonkie: https://github.com/trychonkie/chonkie \\nMultimodal Tools \\n• \\nLLaVA: https://llava-vl.github.io/ \\nVector Databases \\n• \\nQdrant, Milvus, Weaviate \\nSample Datasets \\n• \\nDocVQA: https://docvqa.github.io/ \\n• \\nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \\n• \\nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \\n \\nSubmission \\nWithin 72 hours, submit: \\n• \\nGitHub repository or zip file \\n• \\nShort demo video (3–10 minutes) \\n• \\nLocal setup instructions \\n• \\nOptional: your evaluation report and reflections on architecture decisions \\n', ' \\n72-Hour Technical Challenge \\nMultimodal Enterprise RAG – Leveraging Knowledge Graphs and Hybrid Search \\n \\nObjective \\nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \\nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \\nsearchable knowledge graph, and enables hybrid search using, keyword and vector \\nretrieval (Graph RAG System). \\nYou are expected to begin with evals to define success criteria and to structure your \\narchitecture accordingly. \\n \\nEnterprise Approach Highlights \\nBefore building any ingestion or pipeline logic, define your evaluation framework: \\n• \\nWhat constitutes a \"correct\" response? \\n• \\nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \\n• \\nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \\n• \\nHow will you fail gracefully? \\nStructure your work around a modular and scalable pipeline, including: \\n• \\nInput validation \\n• \\nQuery triage and rewriting \\n• \\nAgent-based retrieval orchestration \\n• \\nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \\nVector Retrieval \\n• \\nAnswer generation and post-processing \\n \\nChallenge Scope \\nBuild a multimodal RAG assistant that: \\n• \\nIngests at least three of the following modalities: text, image, audio, video \\n• \\nExtracts entities and relationships \\n• \\nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \\n• \\nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \\nsophisticated ingestion pipeline \\n• \\nPowers a hybrid search pipeline for fast and reliable access to domain-specific \\nknowledge \\n \\nRequirements \\n1. Evaluation-First Pipeline Design \\n• \\nDefine a minimal test suite using DeepEval or similar \\n• \\nClearly document: \\no Query types (lookup, summarization, semantic linkages) \\no Evaluation goals: retrieval quality, hallucination control, latency \\n• \\nInclude functional unit tests for each module \\n2. Data Ingestion and Preprocessing \\n• \\nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \\n• \\nModal-specific logic: \\no OCR/captioning \\no Transcription \\no Frame extraction and tagging for video \\n• \\nEnrich all outputs with metadata and domain tags \\n3. Entity & Relationship Extraction \\n• \\nUse LLMs to extract structured information \\n• \\nCross-modal linking of the same entity (e.g., “John Smith” in PDF + transcript) \\n• \\nGenerate or infer schema for graph database \\n6. User Interface / Demo \\n• \\nUI or notebook should support: \\no Uploading new files \\no Typing natural language queries \\no Viewing answers with optional graph exploration \\n• \\nLog evaluation output for each query \\n \\nBonus Features \\n• \\nScene detection for video \\n• \\nSentiment detection from text/audio \\n• \\nTopic-based reranking of results \\n• \\nReal-time feedback for query improvement \\n• \\nSecurity-aware design (query restrictions, access control) \\n \\nEvaluation Criteria \\n• \\nEnterprise fit: eval-first mindset, modularity, and clear architecture \\n• \\nPrecision and relevance: does the system retrieve the right context across \\nmodalities? \\n• \\nLatency: fast, efficient retrieval and generation \\n• \\nReliability: graceful failure handling and consistent outputs \\n• \\nMaintainability: clear logic, good documentation, testing \\nResources & Tips \\nThere’s no strict requirement for the type of dataset or topic. Suggested tools and sample \\nsets: \\nFrameworks \\n• \\nAutoGen: https://github.com/microsoft/autogen \\n• \\nCrewAI: https://github.com/joaomdmoura/crewAI \\n• \\nDeepEval: https://github.com/confident-ai/deepeval \\n• \\nArize Phoenix: https://github.com/Arize-ai/phoenix \\n• \\nLangChain: https://www.langchain.com/ \\n• \\nLlamaIndex: https://www.llamaindex.ai/ \\n• \\nChonkie: https://github.com/trychonkie/chonkie \\nMultimodal Tools \\n• \\nLLaVA: https://llava-vl.github.io/ \\nVector Databases \\n• \\nQdrant, Milvus, Weaviate \\nSample Datasets \\n• \\nDocVQA: https://docvqa.github.io/ \\n• \\nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \\n• \\nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \\n \\nSubmission \\nWithin 72 hours, submit: \\n• \\nGitHub repository or zip file \\n• \\nShort demo video (3–10 minutes) \\n• \\nLocal setup instructions \\n• \\nOptional: your evaluation report and reflections on architecture decisions \\n']\n",
      "Graph Results: []\n",
      "Pipeline execution complete. Results saved to results/hybrid_results.json\n"
     ]
    }
   ],
   "source": [
    "!python main.py assets/Aiinfo.mp4 \"mp4 test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "435898ea-d64c-4eca-8204-0ff5b0408c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      " vector_results \n",
      "---\n",
      "Result:\n",
      " graph_results \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"results/hybrid_results.json\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "for r in results:\n",
    "    print(\"Result:\\n\", r, \"\\n---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
