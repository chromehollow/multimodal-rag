{
  "vector_results": [
    " \n72-Hour Technical Challenge \nMultimodal Enterprise RAG \u2013 Leveraging Knowledge Graphs and Hybrid Search \n \nObjective \nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \nsearchable knowledge graph, and enables hybrid search using, keyword and vector \nretrieval (Graph RAG System). \nYou are expected to begin with evals to define success criteria and to structure your \narchitecture accordingly. \n \nEnterprise Approach Highlights \nBefore building any ingestion or pipeline logic, define your evaluation framework: \n\u2022 \nWhat constitutes a \"correct\" response? \n\u2022 \nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \n\u2022 \nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \n\u2022 \nHow will you fail gracefully? \nStructure your work around a modular and scalable pipeline, including: \n\u2022 \nInput validation \n\u2022 \nQuery triage and rewriting \n\u2022 \nAgent-based retrieval orchestration \n\u2022 \nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \nVector Retrieval \n\u2022 \nAnswer generation and post-processing \n \nChallenge Scope \nBuild a multimodal RAG assistant that: \n\u2022 \nIngests at least three of the following modalities: text, image, audio, video \n\u2022 \nExtracts entities and relationships \n\u2022 \nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \n\u2022 \nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \nsophisticated ingestion pipeline \n\u2022 \nPowers a hybrid search pipeline for fast and reliable access to domain-specific \nknowledge \n \nRequirements \n1. Evaluation-First Pipeline Design \n\u2022 \nDefine a minimal test suite using DeepEval or similar \n\u2022 \nClearly document: \no Query types (lookup, summarization, semantic linkages) \no Evaluation goals: retrieval quality, hallucination control, latency \n\u2022 \nInclude functional unit tests for each module \n2. Data Ingestion and Preprocessing \n\u2022 \nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \n\u2022 \nModal-specific logic: \no OCR/captioning \no Transcription \no Frame extraction and tagging for video \n\u2022 \nEnrich all outputs with metadata and domain tags \n3. Entity & Relationship Extraction \n\u2022 \nUse LLMs to extract structured information \n\u2022 \nCross-modal linking of the same entity (e.g., \u201cJohn Smith\u201d in PDF + transcript) \n\u2022 \nGenerate or infer schema for graph database \n6. User Interface / Demo \n\u2022 \nUI or notebook should support: \no Uploading new files \no Typing natural language queries \no Viewing answers with optional graph exploration \n\u2022 \nLog evaluation output for each query \n \nBonus Features \n\u2022 \nScene detection for video \n\u2022 \nSentiment detection from text/audio \n\u2022 \nTopic-based reranking of results \n\u2022 \nReal-time feedback for query improvement \n\u2022 \nSecurity-aware design (query restrictions, access control) \n \nEvaluation Criteria \n\u2022 \nEnterprise fit: eval-first mindset, modularity, and clear architecture \n\u2022 \nPrecision and relevance: does the system retrieve the right context across \nmodalities? \n\u2022 \nLatency: fast, efficient retrieval and generation \n\u2022 \nReliability: graceful failure handling and consistent outputs \n\u2022 \nMaintainability: clear logic, good documentation, testing \nResources & Tips \nThere\u2019s no strict requirement for the type of dataset or topic. Suggested tools and sample \nsets: \nFrameworks \n\u2022 \nAutoGen: https://github.com/microsoft/autogen \n\u2022 \nCrewAI: https://github.com/joaomdmoura/crewAI \n\u2022 \nDeepEval: https://github.com/confident-ai/deepeval \n\u2022 \nArize Phoenix: https://github.com/Arize-ai/phoenix \n\u2022 \nLangChain: https://www.langchain.com/ \n\u2022 \nLlamaIndex: https://www.llamaindex.ai/ \n\u2022 \nChonkie: https://github.com/trychonkie/chonkie \nMultimodal Tools \n\u2022 \nLLaVA: https://llava-vl.github.io/ \nVector Databases \n\u2022 \nQdrant, Milvus, Weaviate \nSample Datasets \n\u2022 \nDocVQA: https://docvqa.github.io/ \n\u2022 \nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \n\u2022 \nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \n \nSubmission \nWithin 72 hours, submit: \n\u2022 \nGitHub repository or zip file \n\u2022 \nShort demo video (3\u201310 minutes) \n\u2022 \nLocal setup instructions \n\u2022 \nOptional: your evaluation report and reflections on architecture decisions \n",
    " \n72-Hour Technical Challenge \nMultimodal Enterprise RAG \u2013 Leveraging Knowledge Graphs and Hybrid Search \n \nObjective \nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \nsearchable knowledge graph, and enables hybrid search using, keyword and vector \nretrieval (Graph RAG System). \nYou are expected to begin with evals to define success criteria and to structure your \narchitecture accordingly. \n \nEnterprise Approach Highlights \nBefore building any ingestion or pipeline logic, define your evaluation framework: \n\u2022 \nWhat constitutes a \"correct\" response? \n\u2022 \nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \n\u2022 \nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \n\u2022 \nHow will you fail gracefully? \nStructure your work around a modular and scalable pipeline, including: \n\u2022 \nInput validation \n\u2022 \nQuery triage and rewriting \n\u2022 \nAgent-based retrieval orchestration \n\u2022 \nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \nVector Retrieval \n\u2022 \nAnswer generation and post-processing \n \nChallenge Scope \nBuild a multimodal RAG assistant that: \n\u2022 \nIngests at least three of the following modalities: text, image, audio, video \n\u2022 \nExtracts entities and relationships \n\u2022 \nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \n\u2022 \nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \nsophisticated ingestion pipeline \n\u2022 \nPowers a hybrid search pipeline for fast and reliable access to domain-specific \nknowledge \n \nRequirements \n1. Evaluation-First Pipeline Design \n\u2022 \nDefine a minimal test suite using DeepEval or similar \n\u2022 \nClearly document: \no Query types (lookup, summarization, semantic linkages) \no Evaluation goals: retrieval quality, hallucination control, latency \n\u2022 \nInclude functional unit tests for each module \n2. Data Ingestion and Preprocessing \n\u2022 \nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \n\u2022 \nModal-specific logic: \no OCR/captioning \no Transcription \no Frame extraction and tagging for video \n\u2022 \nEnrich all outputs with metadata and domain tags \n3. Entity & Relationship Extraction \n\u2022 \nUse LLMs to extract structured information \n\u2022 \nCross-modal linking of the same entity (e.g., \u201cJohn Smith\u201d in PDF + transcript) \n\u2022 \nGenerate or infer schema for graph database \n6. User Interface / Demo \n\u2022 \nUI or notebook should support: \no Uploading new files \no Typing natural language queries \no Viewing answers with optional graph exploration \n\u2022 \nLog evaluation output for each query \n \nBonus Features \n\u2022 \nScene detection for video \n\u2022 \nSentiment detection from text/audio \n\u2022 \nTopic-based reranking of results \n\u2022 \nReal-time feedback for query improvement \n\u2022 \nSecurity-aware design (query restrictions, access control) \n \nEvaluation Criteria \n\u2022 \nEnterprise fit: eval-first mindset, modularity, and clear architecture \n\u2022 \nPrecision and relevance: does the system retrieve the right context across \nmodalities? \n\u2022 \nLatency: fast, efficient retrieval and generation \n\u2022 \nReliability: graceful failure handling and consistent outputs \n\u2022 \nMaintainability: clear logic, good documentation, testing \nResources & Tips \nThere\u2019s no strict requirement for the type of dataset or topic. Suggested tools and sample \nsets: \nFrameworks \n\u2022 \nAutoGen: https://github.com/microsoft/autogen \n\u2022 \nCrewAI: https://github.com/joaomdmoura/crewAI \n\u2022 \nDeepEval: https://github.com/confident-ai/deepeval \n\u2022 \nArize Phoenix: https://github.com/Arize-ai/phoenix \n\u2022 \nLangChain: https://www.langchain.com/ \n\u2022 \nLlamaIndex: https://www.llamaindex.ai/ \n\u2022 \nChonkie: https://github.com/trychonkie/chonkie \nMultimodal Tools \n\u2022 \nLLaVA: https://llava-vl.github.io/ \nVector Databases \n\u2022 \nQdrant, Milvus, Weaviate \nSample Datasets \n\u2022 \nDocVQA: https://docvqa.github.io/ \n\u2022 \nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \n\u2022 \nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \n \nSubmission \nWithin 72 hours, submit: \n\u2022 \nGitHub repository or zip file \n\u2022 \nShort demo video (3\u201310 minutes) \n\u2022 \nLocal setup instructions \n\u2022 \nOptional: your evaluation report and reflections on architecture decisions \n",
    " \n72-Hour Technical Challenge \nMultimodal Enterprise RAG \u2013 Leveraging Knowledge Graphs and Hybrid Search \n \nObjective \nDesign and implement a modular prototype of an Enterprise Retrieval-Augmented \nGeneration (RAG) system that supports text, image, audio, and video ingestion, builds a \nsearchable knowledge graph, and enables hybrid search using, keyword and vector \nretrieval (Graph RAG System). \nYou are expected to begin with evals to define success criteria and to structure your \narchitecture accordingly. \n \nEnterprise Approach Highlights \nBefore building any ingestion or pipeline logic, define your evaluation framework: \n\u2022 \nWhat constitutes a \"correct\" response? \n\u2022 \nWhat types of queries will you support (e.g., factual, lookup, reasoning)? \n\u2022 \nWhat metrics will you track (e.g., hallucination rate, latency, accuracy)? \n\u2022 \nHow will you fail gracefully? \nStructure your work around a modular and scalable pipeline, including: \n\u2022 \nInput validation \n\u2022 \nQuery triage and rewriting \n\u2022 \nAgent-based retrieval orchestration \n\u2022 \nHybrid Search: Structured Graph Traversal + Keyword Filtering + Semantic \nVector Retrieval \n\u2022 \nAnswer generation and post-processing \n \nChallenge Scope \nBuild a multimodal RAG assistant that: \n\u2022 \nIngests at least three of the following modalities: text, image, audio, video \n\u2022 \nExtracts entities and relationships \n\u2022 \nConstructs a searchable knowledge graph (e.g., Neo4j or similar) \n\u2022 \nBuild a searchable Vector Database like Qdrant or Weaviate in parallel with a \nsophisticated ingestion pipeline \n\u2022 \nPowers a hybrid search pipeline for fast and reliable access to domain-specific \nknowledge \n \nRequirements \n1. Evaluation-First Pipeline Design \n\u2022 \nDefine a minimal test suite using DeepEval or similar \n\u2022 \nClearly document: \no Query types (lookup, summarization, semantic linkages) \no Evaluation goals: retrieval quality, hallucination control, latency \n\u2022 \nInclude functional unit tests for each module \n2. Data Ingestion and Preprocessing \n\u2022 \nAccept: .pdf, .txt, .jpg/.png, .mp3/.mp4 \n\u2022 \nModal-specific logic: \no OCR/captioning \no Transcription \no Frame extraction and tagging for video \n\u2022 \nEnrich all outputs with metadata and domain tags \n3. Entity & Relationship Extraction \n\u2022 \nUse LLMs to extract structured information \n\u2022 \nCross-modal linking of the same entity (e.g., \u201cJohn Smith\u201d in PDF + transcript) \n\u2022 \nGenerate or infer schema for graph database \n6. User Interface / Demo \n\u2022 \nUI or notebook should support: \no Uploading new files \no Typing natural language queries \no Viewing answers with optional graph exploration \n\u2022 \nLog evaluation output for each query \n \nBonus Features \n\u2022 \nScene detection for video \n\u2022 \nSentiment detection from text/audio \n\u2022 \nTopic-based reranking of results \n\u2022 \nReal-time feedback for query improvement \n\u2022 \nSecurity-aware design (query restrictions, access control) \n \nEvaluation Criteria \n\u2022 \nEnterprise fit: eval-first mindset, modularity, and clear architecture \n\u2022 \nPrecision and relevance: does the system retrieve the right context across \nmodalities? \n\u2022 \nLatency: fast, efficient retrieval and generation \n\u2022 \nReliability: graceful failure handling and consistent outputs \n\u2022 \nMaintainability: clear logic, good documentation, testing \nResources & Tips \nThere\u2019s no strict requirement for the type of dataset or topic. Suggested tools and sample \nsets: \nFrameworks \n\u2022 \nAutoGen: https://github.com/microsoft/autogen \n\u2022 \nCrewAI: https://github.com/joaomdmoura/crewAI \n\u2022 \nDeepEval: https://github.com/confident-ai/deepeval \n\u2022 \nArize Phoenix: https://github.com/Arize-ai/phoenix \n\u2022 \nLangChain: https://www.langchain.com/ \n\u2022 \nLlamaIndex: https://www.llamaindex.ai/ \n\u2022 \nChonkie: https://github.com/trychonkie/chonkie \nMultimodal Tools \n\u2022 \nLLaVA: https://llava-vl.github.io/ \nVector Databases \n\u2022 \nQdrant, Milvus, Weaviate \nSample Datasets \n\u2022 \nDocVQA: https://docvqa.github.io/ \n\u2022 \nPubLayNet: https://github.com/ibm-aur-nlp/PubLayNet \n\u2022 \nLAION-400M: https://laion.ai/blog/laion-400-open-dataset/ \n \nSubmission \nWithin 72 hours, submit: \n\u2022 \nGitHub repository or zip file \n\u2022 \nShort demo video (3\u201310 minutes) \n\u2022 \nLocal setup instructions \n\u2022 \nOptional: your evaluation report and reflections on architecture decisions \n"
  ],
  "graph_results": []
}